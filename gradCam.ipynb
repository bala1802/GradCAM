{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is a GradCAM?\n",
    "\n",
    "- GradCAM (Gradient-weighted Class Activation Mapping) is a technique used in deep learning and computer vision to visualize the regions of an input image that contribute the most to the predictions made by a neural network model. \n",
    "- It highlights the important regions by generating a heatmap that shows where the model's attention is focused within the image. \n",
    "- GradCAM achieves this by using gradients of the model's output with respect to the input image to determine the importance of each pixel.\n",
    "\n",
    "*Imagine you have a super cool pair of glasses that can show you exactly what your brain is thinking about. Well, in a way, GradCAM is like those glasses, but for computers and pictures.*\n",
    "\n",
    "*You know how when you look at a picture, you can tell what's important in it, like a bright smile on someone's face or a cute puppy in the background? GradCAM helps computers do something similar. It helps them figure out what parts of a picture they are paying the most attention to when they're trying to understand it.*.\n",
    "\n",
    "*Let's say we have a picture of a sunny beach with lots of things happening: people playing in the sand, waves crashing, and colorful umbrellas. When a computer wants to understand what's happening in the picture, it uses something called a `Neural Network` (that's like the computer's brain) to look at all the different parts of the picture*.\n",
    "\n",
    "*GradCAM helps the computer show us where it's focusing the most. It's like shining a bright light on the most interesting parts of the picture. So, if the computer is trying to figure out if there's a dog on the beach, GradCAM would show us which parts of the picture the computer thinks might have the dog – maybe near a group of people or by the water*.\n",
    "\n",
    "In short, GradCAM helps us see what parts of a picture a computer thinks are the most important when it's trying to understand what's in the picture. Just like your brain focuses on the best parts of a story, GradCAM helps computers focus on the best parts of a picture to understand it better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Steps followed in Grad CAM. Let's understand each step involved in the `GradCAM` with respect to the \"Sunny Beach\" example\n",
    "\n",
    "- Capture the output of the *last \"detective\" layer of the computer's brain that looks at the sunny beach picture*.\n",
    "\n",
    "- Calculate how much each *\"detective\" (neuron) in the last layer cares about finding dogs on the beach*, using gradients.\n",
    "\n",
    "- Find the average importance of each *\"detective\" across different parts of the picture to get a list of their interests in finding dogs*.\n",
    "\n",
    "- Multiply the *different parts of the sunny beach picture with their corresponding \"detective\" interests to show which spots are important for detecting dogs*.\n",
    "\n",
    "- Combine these *\"important spots\" to create a map that highlights areas where the computer thinks dogs are likely to be in the picture*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load Dataset\n",
    "\n",
    "- In this Notebook, lets explore the Fashion MNIST datset for understanding the GradCAM\n",
    "\n",
    "- The shape of each image present in the dataset is `28x28`.\n",
    "- The number of classes present in this train data - 60K images and test data - 10K images\n",
    "- The data is converted into PyTorch Tensors.\n",
    "- The data loaders are constructed to pass into the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 23s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 4s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 1, 28, 28]),\n",
       " torch.Size([10000, 1, 28, 28]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = torch.tensor(X_train, dtype=torch.float32),\\\n",
    "                                   torch.tensor(X_test, dtype=torch.float32),\\\n",
    "                                   torch.tensor(Y_train, dtype=torch.long),\\\n",
    "                                   torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "X_train, X_test = X_train.reshape(-1,1,28,28), X_test.reshape(-1,1,28,28)\n",
    "\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n",
    "\n",
    "classes =  Y_train.unique()\n",
    "class_labels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
    "mapping = dict(zip(classes.numpy(), class_labels))\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset  = TensorDataset(X_test , Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Construct a Simple Convolutional Neural Network\n",
    "\n",
    "- The Neural Network has 3 Convolutional Layers and 1 Linear Layer\n",
    "\n",
    "- Convolutional Layer 1 with `48` Output channels\n",
    "- Convolutional Layer 2 with `32` Output channels\n",
    "- Convolutional Layer 3 with `16` Output channels\n",
    "- The Last Linear layer has `10` Output units which is same as the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=12544, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=48, kernel_size=(3,3), padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=48, out_channels=32, kernel_size=(3,3), padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3,3), padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*28*28, len(classes))  \n",
    "        )\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        preds = self.seq(x_batch)\n",
    "        return preds\n",
    "\n",
    "conv_net = ConvNet()\n",
    "conv_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def CalcValLoss(model, loss_func, val_loader):\n",
    "    with torch.no_grad(): ## Prevents calculation of gradients\n",
    "        val_losses = []\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            loss = loss_func(preds, Y_batch)\n",
    "            val_losses.append(loss)\n",
    "        print(\"Valid CategoricalCrossEntropy : {:.3f}\".format(torch.tensor(val_losses).mean()))\n",
    "\n",
    "def MakePredictions(model, loader):\n",
    "    preds, Y_shuffled = [], []\n",
    "    for X_batch, Y_batch in loader:\n",
    "        preds.append(model(X_batch))\n",
    "        Y_shuffled.append(Y_batch)\n",
    "\n",
    "    preds = torch.cat(preds).argmax(axis=-1)\n",
    "    Y_shuffled = torch.cat(Y_shuffled)\n",
    "    return Y_shuffled, preds\n",
    "\n",
    "def TrainModelInBatchesV1(model, loss_func, optimizer, train_loader, val_loader, epochs=5):\n",
    "    for i in range(epochs):\n",
    "        losses = [] ## Record loss of each batch\n",
    "        for X_batch, Y_batch in tqdm(train_loader):\n",
    "            preds = model(X_batch) ## Make Predictions by forward pass through network\n",
    "\n",
    "            loss = loss_func(preds, Y_batch) ## Calculate Loss\n",
    "            losses.append(loss) ## Record Loss\n",
    "\n",
    "            optimizer.zero_grad() ## Zero weights before calculating gradients\n",
    "            loss.backward() ## Calculate Gradients\n",
    "            optimizer.step() ## Update Weights\n",
    "\n",
    "        print(\"Train CategoricalCrossEntropy : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLoss(model, loss_func, val_loader)\n",
    "\n",
    "        Y_test_shuffled, test_preds = MakePredictions(model, val_loader)\n",
    "        val_acc = accuracy_score(Y_test_shuffled, test_preds)\n",
    "        print(\"Val  Accuracy : {:.3f}\".format(val_acc))\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:46<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CategoricalCrossEntropy : 0.438\n",
      "Valid CategoricalCrossEntropy : 0.339\n",
      "Val  Accuracy : 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:46<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CategoricalCrossEntropy : 0.286\n",
      "Valid CategoricalCrossEntropy : 0.308\n",
      "Val  Accuracy : 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:46<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CategoricalCrossEntropy : 0.236\n",
      "Valid CategoricalCrossEntropy : 0.270\n",
      "Val  Accuracy : 0.905\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, RMSprop, Adam\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = torch.tensor(1e-3) # 0.001\n",
    "\n",
    "conv_net = ConvNet()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(params=conv_net.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModelInBatchesV1(conv_net, cross_entropy_loss, optimizer, train_loader, test_loader,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Grad CAM Step-by-Step Process Involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Capture Output Of Last Convolution Layer\n",
    "\n",
    "- In the first step, lets capture the Output of the `Last Convolution Layer`. This is achieved by designing a secondary Convolutional Neural Network using the layers from our original neural network - `ConvNet`\n",
    "\n",
    "- To retrieve the layers of our original network `ConvNet`, let's utilize the `children()` function applied to the network. This will allow us to access the various layers within the network\n",
    "\n",
    "- Let's perform the forward pass on the secondary neural network, with a focus on capturing the `Last Convolution Layer`, a crucial component of our Task. This output is stored as a local member of this new network.\n",
    "\n",
    "- A random sample is selected from our test data, and ran this selected sample through the new network using a `forward pass`\n",
    "\n",
    "- The output of the step 6.1 is an output similar to what our original network would produce - 10 probabilities per sample. It is observed that the shape of the output from the last convolutional layer is `(1, 16, 28, 28)`. The `16` signifies the number of output channels from the convolutional layer, and the batch size of 1 reflects our focus on analyzing a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (5): ReLU()\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=12544, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv_net.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastConvLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LastConvLayerModel, self).__init__()\n",
    "        self.layers = list(list(conv_net.children())[0].children())\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        x = self.layers[0](X_batch)\n",
    "        conv_layer_output = None\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            x = layer(x)\n",
    "            if i == 3:\n",
    "                self.conv_layer_output = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3), tensor(0.9408, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "conv_model = LastConvLayerModel()\n",
    "idx = np.random.choice(range(10000))\n",
    "pred = conv_model(X_test[idx:idx+1])\n",
    "\n",
    "F.softmax(pred, dim=-1).argmax(), F.softmax(pred, dim=-1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.conv_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual    Target : T-shirt/top\n",
      "Predicted Target : Dress\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual    Target : {}\".format(mapping[Y_test[idx].item()]))\n",
    "print(\"Predicted Target : {}\".format(mapping[pred.argmax(dim=-1).item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Take Gradients Of Last Conv Layer Output With Respect to Prediction\n",
    "\n",
    "- Now let's focus on calculating the gradient related to the last convolution layer's output concerning the predicted item. To achieve this, let's utilize the `grad()` function, which is available in the `autograd` sub-module of PyTorch\n",
    "\n",
    "- Two arguments are passed into the `grad()` function.\n",
    "    1.  First input is the `Predicted Probability`\n",
    "    2.  Second input is the output originating from the `Last Convolution` layer.\n",
    "\n",
    "    The `grad()` function takes these inputs and performs Gradient Calculations. Eventually returns the `computed gradients`\n",
    "\n",
    "- The Shape of the `computed gradients` matches that of the last convolutional layer's output - `(1, 16, 28, 28)`\n",
    "\n",
    "For our analysis, we calculated gradients with respect to the maximum predicted probability. However, it's possible to take gradients with respect to any of the 10 probabilities if our objective involves examining activations for other target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import autograd\n",
    "grads = autograd.grad(pred[:, pred.argmax().item()], conv_model.conv_layer_output)\n",
    "grads[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Average Gradients\n",
    "\n",
    "- First, we need to compute the gradients of the output tensor with respect to the `target` class. These gradients are usually obtained through backpropagation. Now we are interested in specific channel's gradients\n",
    "\n",
    "- After obtaining the gradients, we will have a tensor that represents the gradients for each channel of the output. These gradients are computed for each location in the spatial dimensions (`height` and `width`) of the output `feature map`.\n",
    "\n",
    "- The code `grads[0]` corresponds to selecting the gradients for a specific instance in the batch. In this case, it's the first instance. This step is usually done because when we perform backpropagation, we obtain gradients for each instance in the batch. But since we're interested in a specific instance, we choose `grads[0]`.\n",
    "\n",
    "- The `mean` function calculates the average of the selected gradients along the specified dimensions `(0, 2, 3)`. Let's break down what this means\n",
    "    1. `0` refers to the channel dimension. By taking the mean along this dimension, you're averaging the gradients across all the channels for the selected instance.\n",
    "    \n",
    "    2. `2` and `3` refer to the spatial dimensions (`height` and `width`). You're taking the mean across the spatial locations of each channel's gradients.\n",
    "\n",
    "-  The resulting `pooled_grads` tensor will contain the average gradients for each channel. These average gradients give you an idea of how important each channel is for the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3793e-03,  1.7148e-03,  1.5112e-03,  0.0000e+00,  1.3891e-03,\n",
       "         1.8316e-03,  1.7621e-04,  6.9941e-04,  1.6060e-03,  8.2878e-04,\n",
       "         1.6342e-03,  1.3992e-03,  1.1351e-03,  1.5780e-03,  1.1473e-03,\n",
       "        -2.7149e-05])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_grads = grads[0].mean((0,2,3))\n",
    "pooled_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Multiply Convolution Layer Output with Averaged Gradients\n",
    "\n",
    "- Let's combine the Output of the Last Convolutional Layer with the averaged gradients obtained from the section 6.3\n",
    "\n",
    "- Let's iterate through each channel of the output produced by the last convolutional layer. For every individual channel, we performed a multiplication with the corresponding averaged gradient value from the step 6.3.\n",
    "\n",
    "- The outcome of this multiplication is an output array with a shape of `(16, 28, 28)`. This array provides us with a comprehensive view of how the convolutional layer's output interacts with the averaged gradients, highlighting critical regions where these interactions are most significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output = conv_model.conv_layer_output.squeeze()\n",
    "conv_output = F.relu(conv_output)\n",
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(pooled_grads)):\n",
    "    conv_output[i,:,:] *= pooled_grads[i]\n",
    "\n",
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Average Output At Channel Axis To Create Heatmap\n",
    "\n",
    "- This final phase encapsulates our last step, which revolves around generating a heatmap. This heatmap is constructed by averaging the outputs at the channel level from the previous multiplication step.\n",
    "\n",
    "- Specifically, we conducted an averaging operation across all 16 channels present in the output derived earlier. This endeavor yielded a heatmap characterized by a shape of (28, 28). The heatmap is strategically designed to encapsulate activations that play a role in influencing the network's predictions.\n",
    "\n",
    "- As a customary practice to achieve enhanced outcomes, we often choose to normalize the values within the heatmap. This normalization process contributes to refining the resulting heatmap, thereby contributing to more accurate and interpretable insights into the neural network's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap = conv_output.mean(dim=0).squeeze()\n",
    "heatmap = heatmap / torch.max(heatmap)\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Visualize Original Image And Heatmap\n",
    "\n",
    "- Let's visualize our original image and the heatmap next to each other to understand the performance of `GradCAM` algorithm.\n",
    "\n",
    "- From the results, the heatmap highlights activations that contributed to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tx/kp29vzg523qdz2m7wmtms4p40000gn/T/ipykernel_3461/1894943190.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = matplotlib.cm.get_cmap(\"Reds\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_actual_and_heatmap(idx, heatmap):\n",
    "    cmap = matplotlib.cm.get_cmap(\"Reds\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.imshow(X_test[idx].numpy().squeeze());\n",
    "    ax1.set_title(\"Actual\");\n",
    "    ax1.set_xticks([],[]);ax1.set_yticks([],[]);\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(heatmap, cmap=\"Reds\");\n",
    "    ax2.set_title(\"Gradients\");\n",
    "    ax2.set_xticks([],[]);ax2.set_yticks([],[]);\n",
    "\n",
    "plot_actual_and_heatmap(idx, heatmap.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
